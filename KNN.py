import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# üìÇ 2. Charger les donn√©es nettoy√©es avec le bon s√©parateur
df = pd.read_csv('cleaned_data.csv', sep=';')

# V√©rifier les colonnes du fichier
print(df.columns)

# Mise √† jour du nom de la colonne cible
consumption_col = 'Total_Water_Consumption_Billion_Cubic_Meters'

# V√©rification de l'existence de la colonne
if consumption_col not in df.columns:
    raise ValueError(f"La colonne {consumption_col} n'existe pas. V√©rifie les noms de colonnes.")

# Cr√©ation de la cible binaire
median_cons = df[consumption_col].median()
df['High_Consumption'] = (df[consumption_col] > median_cons).astype(int)

# S√©lection des features
features = [col for col in df.columns if col not in [consumption_col, 'High_Consumption', 'Country', 'Year']]
features = [col for col in features if df[col].dtype in ['float64', 'int64']]

# S√©paration des variables X et y
X = df[features]
y = df['High_Consumption']



# 3. S√©paration train-test
print("\n=== √âtape 3: S√©paration des donn√©es ===")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"Taille train: {X_train.shape}, Taille test: {X_test.shape}")

# Normalisation des donn√©es
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Cr√©ation et √©valuation du mod√®le KNN avec K=3
print("\n=== Question 4: Mod√®le KNN avec K=3 ===")
knn3 = KNeighborsClassifier(n_neighbors=3)
knn3.fit(X_train_scaled, y_train)
y_pred_k3 = knn3.predict(X_test_scaled)

print("Pr√©dictions pour les 5 premiers patients:", y_pred_k3[:5])
print("Accuracy (K=3):", accuracy_score(y_test, y_pred_k3))

# 5. Validation crois√©e
print("\n=== Question 5: Validation crois√©e ===")
cv_scores = cross_val_score(knn3, X_train_scaled, y_train, cv=5)
print("Scores de validation crois√©e:", cv_scores)
print("Moyenne des scores:", np.mean(cv_scores))

# 6. Optimisation de K avec GridSearchCV
print("\n=== Questions 6-7: Optimisation de K ===")
param_grid = {'n_neighbors': range(1, 26)}
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)
grid_search.fit(X_train_scaled, y_train)

# Meilleur mod√®le
best_knn = grid_search.best_estimator_
y_pred_best = best_knn.predict(X_test_scaled)

print("Meilleur K:", grid_search.best_params_['n_neighbors'])
print("Meilleure accuracy:", grid_search.best_score_)

# 7. Visualisation des performances
print("\n=== Question 7: Visualisation ===")
plt.figure(figsize=(10, 6))
plt.plot(param_grid['n_neighbors'],
         grid_search.cv_results_['mean_test_score'],
         marker='o')
plt.xlabel('Nombre de voisins (K)')
plt.ylabel('Accuracy')
plt.title('Performance du KNN en fonction de K')
plt.grid()
plt.show()

# 8. √âvaluation finale
print("\n=== Question 8: √âvaluation finale ===")
print("Matrice de confusion:")
print(confusion_matrix(y_test, y_pred_best))
print("\nRapport de classification:")
print(classification_report(y_test, y_pred_best))

# Visualisation des donn√©es
print("\n=== Visualisation suppl√©mentaire ===")
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.countplot(x='High_Consumption', data=df)
plt.title('Distribution des classes')

plt.subplot(1, 2, 2)
sns.heatmap(df[features].corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Matrice de corr√©lation')

plt.tight_layout()
plt.show()